{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tqdm import tqdm\n",
        "\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.DEBUG)"
      ],
      "metadata": {
        "id": "GAeFCrp4PQER"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfDbyNfIPUDU",
        "outputId": "b6e9f244-e2c7-4cad-ddb2-c72a10c3f74a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "v1rtGD4yPIyY"
      },
      "outputs": [],
      "source": [
        "# Function to normalize and preprocess the magnitude and phase\n",
        "def preprocess_spectrograms(magnitude_path, phase_path):\n",
        "    # Load the magnitude and phase\n",
        "    magnitude = np.load(magnitude_path)\n",
        "    phase = np.load(phase_path)\n",
        "\n",
        "    # Original shape\n",
        "    original_shape = magnitude.shape\n",
        "\n",
        "    # Normalize magnitude\n",
        "    scaler = MinMaxScaler()\n",
        "    magnitude = scaler.fit_transform(magnitude.reshape(-1, magnitude.shape[-1])).reshape(magnitude.shape)\n",
        "\n",
        "    # Normalize phase\n",
        "    phase = (phase + np.pi) / (2 * np.pi)  # Normalize to [0, 1]\n",
        "\n",
        "    # Concatenate magnitude and phase into a 2-channel image\n",
        "    spectrogram_image = np.stack([magnitude, phase], axis=0)\n",
        "    return spectrogram_image, scaler, original_shape\n",
        "\n",
        "def denormalize_spectrograms(spectrogram_image, scaler, original_shape):\n",
        "    # Separate magnitude and phase\n",
        "    magnitude = spectrogram_image[0]\n",
        "    phase = spectrogram_image[1]\n",
        "\n",
        "    # Denormalize magnitude\n",
        "    magnitude = scaler.inverse_transform(magnitude.reshape(-1, magnitude.shape[-1])).reshape(magnitude.shape)\n",
        "\n",
        "    # Denormalize phase\n",
        "    phase = phase * 2 * np.pi - np.pi\n",
        "\n",
        "    return magnitude, phase\n",
        "\n",
        "class ConvVAE(nn.Module):\n",
        "    def __init__(self, in_channels, latent_dim, img_height, img_width, device):\n",
        "        super(ConvVAE, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.img_height = img_height\n",
        "        self.img_width = img_width\n",
        "        self.device = device\n",
        "\n",
        "        # Encoder\n",
        "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=4, stride=2, padding=1)  # Output: (32, img_height/2, img_width/2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)          # Output: (64, img_height/4, img_width/4)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)         # Output: (128, img_height/8, img_width/8)\n",
        "\n",
        "        # Compute the size of the flattened layer\n",
        "        flattened_size = 128 * (img_height // 8) * (img_width // 8)\n",
        "        self.fc_mean = nn.Linear(flattened_size, latent_dim)\n",
        "        self.fc_log_var = nn.Linear(flattened_size, latent_dim)\n",
        "\n",
        "        # Decoder\n",
        "        self.fc_decode = nn.Linear(latent_dim, flattened_size)\n",
        "        self.deconv1 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)  # Output: (64, img_height/4, img_width/4)\n",
        "        self.deconv2 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)   # Output: (32, img_height/2, img_width/2)\n",
        "        self.deconv3 = nn.ConvTranspose2d(32, in_channels, kernel_size=4, stride=2, padding=1)  # Output: (in_channels, img_height, img_width)\n",
        "\n",
        "    def encode(self, x):\n",
        "        logging.debug(f\"Input shape to encoder: {x.shape}\")\n",
        "        e = F.relu(self.conv1(x))\n",
        "        logging.debug(f\"Shape after conv1: {e.shape}\")\n",
        "        e = F.relu(self.conv2(e))\n",
        "        logging.debug(f\"Shape after conv2: {e.shape}\")\n",
        "        e = F.relu(self.conv3(e))\n",
        "        logging.debug(f\"Shape after conv3: {e.shape}\")\n",
        "        e = e.view(e.size(0), -1)\n",
        "        logging.debug(f\"Shape after flattening: {e.shape}\")\n",
        "        z_mean = self.fc_mean(e)\n",
        "        z_log_var = self.fc_log_var(e)\n",
        "        logging.debug(f\"Shape of z_mean: {z_mean.shape}\")\n",
        "        logging.debug(f\"Shape of z_log_var: {z_log_var.shape}\")\n",
        "        return z_mean, z_log_var\n",
        "\n",
        "    def reparameterize(self, mean, log_var):\n",
        "        epsilon = torch.randn_like(mean)\n",
        "        return mean + torch.exp(0.5 * log_var) * epsilon\n",
        "\n",
        "    def decode(self, z):\n",
        "        logging.debug(f\"Input shape to decoder: {z.shape}\")\n",
        "        d = self.fc_decode(z)\n",
        "        logging.debug(f\"Shape after fc_decode: {d.shape}\")\n",
        "        d = d.view(d.size(0), 128, self.img_height // 8, self.img_width // 8)  # Reshape\n",
        "        logging.debug(f\"Shape after reshape: {d.shape}\")\n",
        "        d = F.relu(self.deconv1(d))\n",
        "        logging.debug(f\"Shape after deconv1: {d.shape}\")\n",
        "        d = F.relu(self.deconv2(d))\n",
        "        logging.debug(f\"Shape after deconv2: {d.shape}\")\n",
        "        outputs = torch.sigmoid(self.deconv3(d))\n",
        "        logging.debug(f\"Shape after deconv3: {outputs.shape}\")\n",
        "        return outputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        z_mean, z_log_var = self.encode(x)\n",
        "        z = self.reparameterize(z_mean, z_log_var)\n",
        "        x_reconstructed = self.decode(z)\n",
        "        return x_reconstructed, z_mean, z_log_var\n",
        "\n",
        "    def compute_loss(self, x, x_reconstructed, z_mean, z_log_var):\n",
        "        # Debugging logs\n",
        "        logging.debug(f\"Shape of input: {x.shape}\")\n",
        "        logging.debug(f\"Shape of reconstructed: {x_reconstructed.shape}\")\n",
        "        logging.debug(f\"x min: {x.min().item()}, x max: {x.max().item()}\")\n",
        "        logging.debug(f\"x_reconstructed min: {x_reconstructed.min().item()}, x_reconstructed max: {x_reconstructed.max().item()}\")\n",
        "        # Compute the reconstruction loss using MSE\n",
        "        reconstruction_loss = F.mse_loss(x_reconstructed, x, reduction='sum')\n",
        "        kl_loss = -0.5 * torch.mean(z_log_var - torch.square(z_mean) - torch.exp(z_log_var) + 1)\n",
        "        total_loss = reconstruction_loss + kl_loss\n",
        "        # More debugging logs\n",
        "        logging.debug(f\"Reconstruction loss: {reconstruction_loss.item()}\")\n",
        "        logging.debug(f\"KL loss: {kl_loss.item()}\")\n",
        "        logging.debug(f\"Total loss: {total_loss.item()}\")\n",
        "        return total_loss\n",
        "\n",
        "    def training_step(self, x):\n",
        "        self.optimizer.zero_grad()\n",
        "        x_reconstructed, z_mean, z_log_var = self(x)\n",
        "        loss = self.compute_loss(x, x_reconstructed, z_mean, z_log_var)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss.item()\n",
        "\n",
        "    def generate_images(self, num_samples):\n",
        "        with torch.no_grad():\n",
        "            z = torch.randn(num_samples, self.latent_dim).to(self.device)\n",
        "            return self.decode(z).cpu().numpy()\n",
        "\n",
        "    def plot_images(self, training_images, generated_images):\n",
        "        num_images = min(5, training_images.shape[0], generated_images.shape[0])\n",
        "        fig, axs = plt.subplots(2, num_images, figsize=(15, 6))\n",
        "\n",
        "        for i in range(num_images):\n",
        "            # Plot training images\n",
        "            axs[0, i].imshow(training_images[i].transpose(1, 2, 0))\n",
        "            axs[0, i].set_title('Training Image')\n",
        "            axs[0, i].axis('off')\n",
        "\n",
        "            # Plot generated images\n",
        "            axs[1, i].imshow(generated_images[i].transpose(1, 2, 0))\n",
        "            axs[1, i].set_title('Generated Image')\n",
        "            axs[1, i].axis('off')\n",
        "\n",
        "        plt.suptitle('Training vs Generated Images')\n",
        "        plt.savefig('CVAE.png')\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_paths(clean_data_dir):\n",
        "    image_paths = []\n",
        "    for filename in os.listdir(clean_data_dir):\n",
        "        if filename.endswith('_magnitude.npy'):\n",
        "            ex = filename.split('_')[0]\n",
        "            magnitude_path = os.path.join(clean_data_dir, filename)\n",
        "            phase_path = os.path.join(clean_data_dir, f'{ex}_phase.npy')\n",
        "            if os.path.exists(phase_path):\n",
        "                image_paths.append((magnitude_path, phase_path))\n",
        "    return image_paths\n",
        "\n",
        "class SpectrogramDataset(Dataset):\n",
        "    def __init__(self, image_paths):\n",
        "        self.image_paths = image_paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        magnitude_path, phase_path = self.image_paths[idx]\n",
        "        spectrogram_image, scaler, original_shape = preprocess_spectrograms(magnitude_path, phase_path)\n",
        "\n",
        "        # Remove the last row and column from the spectrogram image\n",
        "        spectrogram_image_cropped = spectrogram_image[:, :-1, :-1]  # Assumes spectrogram_image has shape (channels, height, width)\n",
        "\n",
        "        return torch.tensor(spectrogram_image_cropped, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "aA3kz93kaTbT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_data_dir = 'drive/MyDrive/spectrogram_npy'\n",
        "batch_size = 8\n",
        "latent_dim = 32\n",
        "epochs = 10"
      ],
      "metadata": {
        "id": "01yqo2jeVToK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all image paths\n",
        "image_paths = get_image_paths(clean_data_dir)\n",
        "dataset = SpectrogramDataset(image_paths)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "v5dQDubta0WV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Image dimensions\n",
        "img_height = 128\n",
        "img_width = 10328\n",
        "\n",
        "# Create the model instance\n",
        "vae = ConvVAE(in_channels=2, latent_dim=latent_dim, img_height=img_height, img_width=img_width, device=device).to(device)\n",
        "vae.optimizer = optim.Adam(vae.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "ANtHzzFAVRfF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop with loss history\n",
        "loss_history = []\n",
        "training_images_batch = None\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0\n",
        "    print(f\"epoch {epoch}\")\n",
        "    for batch_x in tqdm(dataloader):\n",
        "        batch_x = batch_x.to(device)\n",
        "        loss = vae.training_step(batch_x)\n",
        "        epoch_loss += loss\n",
        "\n",
        "        # Save a batch of training images for plotting\n",
        "        if training_images_batch is None:\n",
        "            training_images_batch = batch_x.cpu().numpy()\n",
        "\n",
        "    epoch_loss /= len(dataloader)\n",
        "    loss_history.append(epoch_loss)\n",
        "    if epoch % 1 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {epoch_loss}')"
      ],
      "metadata": {
        "id": "smJFDIL_R9gM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7356845-e8a3-4391-a94c-d70497bae908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|‚ñè         | 2/88 [01:16<53:22, 37.24s/it]  "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot loss history\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(loss_history, label='Training Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss History')\n",
        "plt.legend()\n",
        "plt.savefig('loss_history.png')\n",
        "plt.close()\n",
        "\n",
        "# Generate and plot images\n",
        "num_samples = 5\n",
        "generated_images = vae.generate_images(num_samples)\n",
        "vae.plot_images(training_images_batch[:num_samples], generated_images)\n"
      ],
      "metadata": {
        "id": "ISqallIdSZcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model's state dictionary\n",
        "torch.save(vae.state_dict(), 'vae_model.pth')\n",
        "print(\"Model saved to 'vae_model.pth'\")"
      ],
      "metadata": {
        "id": "X0KlWI26pysb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}